{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08vsB3m4LmNA",
        "outputId": "c4715c66-6c24-42b1-d110-03d1bfceb571"
      },
      "outputs": [],
      "source": [
        "!pip install decord\n",
        "!pip install wandb\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms, models\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import random\n",
        "from decord import VideoReader\n",
        "from decord import cpu\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from scipy.stats import spearmanr\n",
        "import torch.nn.functional as F\n",
        "import wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXjXE840UrJ9",
        "outputId": "bd69c399-86b6-405e-dee5-1278caec420d"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle --upgrade\n",
        "os.environ['KAGGLE_USERNAME'] = \"XXXXX\"\n",
        "os.environ['KAGGLE_KEY'] = \"XXXXXXXXXXXXXX\"\n",
        "\n",
        "!kaggle datasets download sarthakjain004/msvd-clips\n",
        "!unzip msvd-clips.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyhnrAC_syJX",
        "outputId": "f03cb450-0cd4-4c25-99ab-b1424ca768d9"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download julianschelb/wordsim353-crowd\n",
        "!unzip wordsim353-crowd.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQ0tI7B9lTxI"
      },
      "outputs": [],
      "source": [
        "class CFG:\n",
        "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    tokenizer=DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "    val_ratio=0.2\n",
        "    batch_size=8\n",
        "    lr=1e-5\n",
        "    weight_decay=1e-4\n",
        "    train_epochs=12\n",
        "\n",
        "    video_dropout=0.3\n",
        "    text_dropout=0.3\n",
        "    video_trainable=True\n",
        "    text_trainable=True\n",
        "\n",
        "    image_mode=False\n",
        "    seed=42\n",
        "    num_captions=3\n",
        "    num_frames=16\n",
        "    temperature=0.07\n",
        "\n",
        "    cache_dir=\"cache\"\n",
        "    video_dir=\"YouTubeClips\"\n",
        "    annotation_file=\"annotations.txt\"\n",
        "    simverb_path=\"simverb-3500.csv\"\n",
        "    wordsim_path=\"wordsim353crowd.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "mcEuGEuJq7EN",
        "outputId": "0d78dcb2-38e3-4102-aecd-9700077ddf46"
      },
      "outputs": [],
      "source": [
        "!wandb login\n",
        "config_dict = {\n",
        "    key: str(value) if isinstance(value, (torch.device,)) else value\n",
        "    for key, value in CFG.__dict__.items()\n",
        "    if not key.startswith(\"__\")\n",
        "}\n",
        "config_dict\n",
        "wandb.init(project=\"CLIP\", config=config_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UP2SWKu5so_d",
        "outputId": "5fbfcf17-02e1-4479-f164-5fd50958ae00"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def worker_init_fn(worker_id):\n",
        "    seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "\n",
        "set_seed(CFG.seed)\n",
        "\n",
        "generator = torch.Generator()\n",
        "generator.manual_seed(CFG.seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wPd6Cg1rmrb"
      },
      "outputs": [],
      "source": [
        "def evaluate_benchmarks(model):\n",
        "  text_encoder = model.text_encoder\n",
        "  text_encoder.eval()\n",
        "\n",
        "  def get_embedding(word):\n",
        "    tokens = tokenizer(word, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=30)\n",
        "    tokens = {k: v.to(CFG.device) for k, v in tokens.items()}\n",
        "    with torch.no_grad():\n",
        "        embedding = text_encoder(tokens)\n",
        "    return embedding.squeeze(0)\n",
        "\n",
        "  def cosine_similarity(emb1, emb2):\n",
        "    emb1 = F.normalize(emb1, dim=0)\n",
        "    emb2 = F.normalize(emb2, dim=0)\n",
        "    return torch.dot(emb1, emb2).item()\n",
        "\n",
        "  results = {}\n",
        "  df_verb = pd.read_csv(CFG.simverb_path)\n",
        "  df_word = pd.read_csv(CFG.wordsim_path)\n",
        "\n",
        "  model_sims = []\n",
        "  human_sims = []\n",
        "  for _, row in df_verb.iterrows():\n",
        "      emb1 = get_embedding(row[\"word1\"])\n",
        "      emb2 = get_embedding(row[\"word2\"])\n",
        "      sim = cosine_similarity(emb1, emb2)\n",
        "      model_sims.append(sim)\n",
        "      human_sims.append(row[\"similarity\"])\n",
        "  corr, p_value = spearmanr(human_sims, model_sims)\n",
        "  results[\"simverb_corr\"] = corr\n",
        "\n",
        "  model_sims = []\n",
        "  human_sims = []\n",
        "  for _, row in df_word.iterrows():\n",
        "      emb1 = get_embedding(row[\"Word 1\"])\n",
        "      emb2 = get_embedding(row[\"Word 2\"])\n",
        "      sim = cosine_similarity(emb1, emb2)\n",
        "      model_sims.append(sim)\n",
        "      human_sims.append(row[\"Human (Mean)\"])\n",
        "  corr, p_value = spearmanr(human_sims, model_sims)\n",
        "  results[\"wordsim_corr\"] = corr\n",
        "\n",
        "  return results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzkB8Q9dJdzA"
      },
      "outputs": [],
      "source": [
        "class MSVDVideoCaptionDataset(Dataset):\n",
        "    def __init__(self, video_dir, annotation_file, tokenizer, cache_dir=\"cache\", num_frames=8, transform=None, video_ids=None, single_frame=False, num_captions=5, seed=42):\n",
        "        self.video_dir = video_dir\n",
        "        self.transform = transform\n",
        "        self.num_frames = num_frames\n",
        "        self.tokenizer = tokenizer\n",
        "        self.single_frame = single_frame\n",
        "        self.cache_dir = cache_dir\n",
        "        self.num_captions = num_captions\n",
        "\n",
        "        os.makedirs(cache_dir, exist_ok=True)\n",
        "        rng = random.Random(seed)\n",
        "\n",
        "        self.captions_by_video = {}\n",
        "        with open(annotation_file, 'r') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "                video_id = parts[0] + \".avi\"\n",
        "                caption = \" \".join(parts[1:])\n",
        "                if (video_ids is None) or (video_id in video_ids):\n",
        "                    if video_id not in self.captions_by_video:\n",
        "                        self.captions_by_video[video_id] = []\n",
        "                    self.captions_by_video[video_id].append(caption)\n",
        "\n",
        "        self.samples = [(vid, caption) for vid, captions in self.captions_by_video.items() for caption in rng.sample(captions, min(len(captions), self.num_captions))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_id, caption = self.samples[idx]\n",
        "        base_cache_path = os.path.join(self.cache_dir, video_id[:-4])\n",
        "        os.makedirs(base_cache_path, exist_ok=True)\n",
        "\n",
        "        if self.single_frame:\n",
        "            cache_path = os.path.join(base_cache_path, \"frame.pt\")\n",
        "            if os.path.exists(cache_path):\n",
        "                frames = torch.load(cache_path)\n",
        "            else:\n",
        "                video_path = os.path.join(self.video_dir, video_id)\n",
        "                vr = VideoReader(video_path, ctx=cpu(0))\n",
        "                total_frames = len(vr)\n",
        "                frame_idx = total_frames // 2\n",
        "                frame = Image.fromarray(vr[frame_idx].asnumpy())\n",
        "                if self.transform:\n",
        "                    frame = self.transform(frame)\n",
        "                frames = frame.unsqueeze(0)\n",
        "                torch.save(frames, cache_path)\n",
        "        else:\n",
        "            cache_path = os.path.join(base_cache_path, f\"frames_{self.num_frames}.pt\")\n",
        "            if os.path.exists(cache_path):\n",
        "                frames = torch.load(cache_path)\n",
        "            else:\n",
        "                video_path = os.path.join(self.video_dir, video_id)\n",
        "                vr = VideoReader(video_path, ctx=cpu(0))\n",
        "                total_frames = len(vr)\n",
        "                if total_frames >= self.num_frames:\n",
        "                    start = int(0.2 * total_frames)\n",
        "                    end = int(0.8 * total_frames)\n",
        "                    indices = np.linspace(start, end - 1, self.num_frames, dtype=int)\n",
        "                else:\n",
        "                    indices = np.linspace(0, total_frames - 1, self.num_frames, dtype=int)\n",
        "                    indices = np.clip(indices, 0, total_frames - 1)\n",
        "                frames_np = vr.get_batch(indices).asnumpy()\n",
        "                frames = [Image.fromarray(frame) for frame in frames_np]\n",
        "                if self.transform:\n",
        "                    frames = [self.transform(img) for img in frames]\n",
        "                frames = torch.stack(frames)\n",
        "                torch.save(frames, cache_path)\n",
        "\n",
        "        tokens = self.tokenizer(caption, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=30)\n",
        "        return frames, tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfSp1oYALl5B"
      },
      "outputs": [],
      "source": [
        "class VideoEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim=512, trainable=True, dropout=0.3):\n",
        "        super(VideoEncoder, self).__init__()\n",
        "        cnn = models.resnet18(pretrained=True)\n",
        "        if not trainable:\n",
        "            for param in cnn.parameters():\n",
        "                param.requires_grad = False\n",
        "        self.backbone = nn.Sequential(*list(cnn.children())[:-1])\n",
        "        self.fc = nn.Linear(512, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.frame_score_layer = nn.Linear(embed_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C, H, W = x.size()\n",
        "        x = x.view(B * T, C, H, W)\n",
        "        features = self.backbone(x).squeeze(-1).squeeze(-1)\n",
        "        features = self.fc(features)\n",
        "        features = features.view(B, T, -1)\n",
        "\n",
        "        scores = self.frame_score_layer(features).squeeze(-1)\n",
        "        weights = torch.softmax(scores, dim=1)\n",
        "\n",
        "        weighted_features = (features * weights.unsqueeze(-1)).sum(dim=1)\n",
        "        return self.dropout(weighted_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUVvhUB7Llve"
      },
      "outputs": [],
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim=512, pretrained=True, trainable=True, dropout=0.3):\n",
        "        super(TextEncoder, self).__init__()\n",
        "        if pretrained:\n",
        "            self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        else:\n",
        "            self.bert = DistilBertModel(config=DistilBertConfig())\n",
        "\n",
        "        if not trainable:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        self.fc = nn.Linear(self.bert.config.hidden_size, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        output = self.bert(**tokens).last_hidden_state[:, 0]\n",
        "        return self.dropout(self.fc(output))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vym_R0AMLljz"
      },
      "outputs": [],
      "source": [
        "class CLIP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CLIP, self).__init__()\n",
        "        self.video_encoder = VideoEncoder(trainable=CFG.video_trainable, dropout=CFG.video_dropout)\n",
        "        self.text_encoder = TextEncoder(trainable=CFG.text_trainable, dropout=CFG.text_dropout)\n",
        "\n",
        "    def forward(self, video_frames, text_tokens):\n",
        "        video_embed = self.video_encoder(video_frames)\n",
        "        text_embed = self.text_encoder(text_tokens)\n",
        "        return video_embed, text_embed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l38johj_LlUE"
      },
      "outputs": [],
      "source": [
        "def contrastive_loss_hard(video_embed, text_embed, temperature=0.07):\n",
        "    video_embed = nn.functional.normalize(video_embed, dim=-1)\n",
        "    text_embed = nn.functional.normalize(text_embed, dim=-1)\n",
        "\n",
        "    logits = video_embed @ text_embed.T / temperature\n",
        "    labels = torch.arange(len(video_embed)).to(video_embed.device)\n",
        "\n",
        "    loss_i2t = nn.CrossEntropyLoss()(logits, labels)\n",
        "    loss_t2i = nn.CrossEntropyLoss()(logits.T, labels)\n",
        "\n",
        "    return (loss_i2t + loss_t2i) / 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dZQRTVkLwRh"
      },
      "outputs": [],
      "source": [
        "def train_loop(model, train_loader, val_loader, optimizer, device, num_epochs=6):\n",
        "    os.makedirs(\"output\", exist_ok=True)\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        model.text_encoder.train()\n",
        "        total_loss = 0\n",
        "        pbar = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\")\n",
        "        for video_frames, text_tokens in pbar:\n",
        "            video_frames = video_frames.to(device)\n",
        "            text_tokens = {k: v.squeeze(1).to(device) for k, v in text_tokens.items()}\n",
        "\n",
        "            video_embed, text_embed = model(video_frames, text_tokens)\n",
        "            loss = contrastive_loss_hard(video_embed, text_embed, temperature=CFG.temperature)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            pbar.set_postfix({\"batch_loss\": f\"{loss.item():.4f}\"})\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}\")\n",
        "\n",
        "        if val_loader:\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "            pbar = tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}\")\n",
        "            with torch.no_grad():\n",
        "                for video_frames, text_tokens in pbar:\n",
        "                    video_frames = video_frames.to(device)\n",
        "                    text_tokens = {k: v.squeeze(1).to(device) for k, v in text_tokens.items()}\n",
        "                    video_embed, text_embed = model(video_frames, text_tokens)\n",
        "                    loss = contrastive_loss_hard(video_embed, text_embed, temperature=CFG.temperature)\n",
        "                    val_loss += loss.item()\n",
        "                    pbar.set_postfix({\"batch_loss\": f\"{loss.item():.4f}\"})\n",
        "            avg_val_loss = val_loss / len(val_loader)\n",
        "            print(f\"Epoch {epoch+1}: Val Loss = {avg_val_loss:.4f}\")\n",
        "\n",
        "        if avg_val_loss <= best_loss:\n",
        "          best_loss = avg_val_loss\n",
        "          torch.save(model.state_dict(), f\"output/clip_best.pt\")\n",
        "          print(\"Saved best model\")\n",
        "\n",
        "        results = evaluate_benchmarks(model)\n",
        "        print(\"wordsim: \", results[\"wordsim_corr\"])\n",
        "        print(\"simverb: \", results[\"simverb_corr\"])\n",
        "\n",
        "        wandb.log({\n",
        "          \"epoch\": epoch+1,\n",
        "          \"train_loss\": avg_train_loss,\n",
        "          \"val_loss\": avg_val_loss,\n",
        "          \"wordsim_corr\": results[\"wordsim_corr\"],\n",
        "          \"simverb_corr\": results[\"simverb_corr\"]\n",
        "        })\n",
        "        wandb.save(\"output/clip_best.pt\")\n",
        "\n",
        "    wandb.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BWfQ0E9LweZ"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IasS_CDpaRcc"
      },
      "outputs": [],
      "source": [
        "tokenizer = CFG.tokenizer\n",
        "all_videos = set()\n",
        "with open(CFG.annotation_file, 'r') as f:\n",
        "    for line in f:\n",
        "        video_id = line.strip().split()[0] + \".avi\"\n",
        "        all_videos.add(video_id)\n",
        "\n",
        "all_videos = list(all_videos)\n",
        "random.shuffle(all_videos)\n",
        "val_ratio = CFG.val_ratio\n",
        "val_size = int(len(all_videos) * val_ratio)\n",
        "val_videos = set(all_videos[:val_size])\n",
        "train_videos = set(all_videos[val_size:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_xYr4Ue_mk0"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "train_dataset = MSVDVideoCaptionDataset(\n",
        "  video_dir=CFG.video_dir,\n",
        "  annotation_file=CFG.annotation_file,\n",
        "  tokenizer=tokenizer,\n",
        "  cache_dir=CFG.cache_dir,\n",
        "  num_frames=CFG.num_frames,\n",
        "  transform=transform,\n",
        "  video_ids=train_videos,\n",
        "  single_frame=CFG.image_mode,\n",
        "  num_captions=CFG.num_captions,\n",
        "  seed=CFG.seed\n",
        ")\n",
        "\n",
        "val_dataset = MSVDVideoCaptionDataset(\n",
        "  video_dir=CFG.video_dir,\n",
        "  annotation_file=CFG.annotation_file,\n",
        "  tokenizer=tokenizer,\n",
        "  cache_dir=CFG.cache_dir,\n",
        "  num_frames=CFG.num_frames,\n",
        "  transform=transform,\n",
        "  video_ids=val_videos,\n",
        "  single_frame=CFG.image_mode,\n",
        "  num_captions=CFG.num_captions,\n",
        "  seed=CFG.seed\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDvuU40wFr5B",
        "outputId": "4cc98011-45db-473f-e848-a760cea2dd18"
      },
      "outputs": [],
      "source": [
        "print(len(train_dataset), len(val_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1ug-vAGF3Rs",
        "outputId": "5c11d8a7-aa31-4b33-a624-ded69c725b84"
      },
      "outputs": [],
      "source": [
        "total_tokens = 0\n",
        "\n",
        "for _, tokens in train_dataset:\n",
        "    input_ids = tokens[\"input_ids\"].squeeze(0)\n",
        "    token_count = (input_ids != 0).sum().item()\n",
        "    total_tokens += token_count\n",
        "\n",
        "print(f\"Number if tokens in train data: {total_tokens}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhXTkNdX_ysg",
        "outputId": "788f0a7d-ca93-4917-e3ce-52e5401abac6"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True, num_workers=4, worker_init_fn=worker_init_fn, generator=generator)\n",
        "val_loader = DataLoader(val_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=4, worker_init_fn=worker_init_fn, generator=generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHo5Jd1rRaPg",
        "outputId": "65f2c38a-c052-411b-d46b-82916ae8df8d"
      },
      "outputs": [],
      "source": [
        "len(train_loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8UDq_Qc_29z",
        "outputId": "49704c63-bbe0-4780-d2f7-106a9ff28d61"
      },
      "outputs": [],
      "source": [
        "device = CFG.device\n",
        "model = CLIP().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nX05avxU_6Vd",
        "outputId": "09094b03-abe5-412d-df22-7fb0ca0c8a08"
      },
      "outputs": [],
      "source": [
        "train_loop(model, train_loader, val_loader, optimizer, device, num_epochs=CFG.train_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ggRu991AI_I"
      },
      "source": [
        "# Evaluation\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
