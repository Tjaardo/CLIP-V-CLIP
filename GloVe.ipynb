{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YS4yZdij7C-6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "import re\n",
        "import random\n",
        "import wandb\n",
        "import torch\n",
        "import pandas as pd\n",
        "from scipy.stats import spearmanr\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMaTmWSLCiw5",
        "outputId": "843212e9-15b0-4f6b-cf34-38915766c226"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle --upgrade\n",
        "os.environ['KAGGLE_USERNAME'] = \"XXXXX\"\n",
        "os.environ['KAGGLE_KEY'] = \"XXXXXXXXXXXXXX\"\n",
        "!kaggle datasets download julianschelb/wordsim353-crowd\n",
        "!unzip wordsim353-crowd.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_mQBm6c8WgX"
      },
      "outputs": [],
      "source": [
        "class CFG:\n",
        "  num_captions=1\n",
        "  seed=42\n",
        "\n",
        "  window_size=2\n",
        "  lr=1e-4\n",
        "  train_epochs=1000\n",
        "\n",
        "  emb_size=512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yR3yoncj7WzY"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(CFG.seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFRHQESo8bDx"
      },
      "outputs": [],
      "source": [
        "def load_captions(file_path, n):\n",
        "    captions_by_id = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(' ', 1)\n",
        "            if len(parts) != 2:\n",
        "                continue\n",
        "            vid_id, caption = parts\n",
        "            captions_by_id.setdefault(vid_id, []).append(caption)\n",
        "    selected_captions = []\n",
        "    for vid_id, captions in captions_by_id.items():\n",
        "        sampled = random.sample(captions, min(n, len(captions)))\n",
        "        tokenized = [caption.strip().split() for caption in sampled]\n",
        "        selected_captions.extend(tokenized)\n",
        "\n",
        "    return selected_captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTW1Aj5aBgqA"
      },
      "outputs": [],
      "source": [
        "corpus = load_captions(\"annotations.txt\", CFG.num_captions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTfCjsh0CmX-",
        "outputId": "f94436bc-30d9-4994-ebe0-032f25b75486"
      },
      "outputs": [],
      "source": [
        "len(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9bXWK1i9lrW"
      },
      "outputs": [],
      "source": [
        "def evaluate_benchmarks(model, word2idx):\n",
        "\n",
        "  def get_embedding(word, model, word2idx):\n",
        "    if word not in word2idx:\n",
        "        raise KeyError(f\"word '{word}' not in vocab\")\n",
        "    idx = torch.tensor([word2idx[word]], device=\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        emb = model.wi(idx) + model.wj(idx)\n",
        "    return emb.squeeze(0)\n",
        "\n",
        "  def cosine_similarity(emb1, emb2):\n",
        "    emb1 = emb1.unsqueeze(0)\n",
        "    emb2 = emb2.unsqueeze(0)\n",
        "    return F.cosine_similarity(emb1, emb2).item()\n",
        "\n",
        "  simverb_df = pd.read_csv(\"simverb-3500.csv\")\n",
        "  wordsim_df = pd.read_csv(\"wordsim353crowd.csv\")\n",
        "  model.eval()\n",
        "  results = {}\n",
        "\n",
        "  human_scores = []\n",
        "  model_scores = []\n",
        "\n",
        "  for _, row in simverb_df.iterrows():\n",
        "      w1, w2, human_sim = row['word1'], row['word2'], row['similarity']\n",
        "\n",
        "      if w1 in word2idx and w2 in word2idx:\n",
        "          emb1 = get_embedding(w1, model, word2idx)\n",
        "          emb2 = get_embedding(w2, model, word2idx)\n",
        "\n",
        "          sim = cosine_similarity(emb1, emb2)\n",
        "          human_scores.append(human_sim)\n",
        "          model_scores.append(sim)\n",
        "      else:\n",
        "          pass\n",
        "\n",
        "  corr, _ = spearmanr(human_scores, model_scores)\n",
        "  results[\"simverb\"] = corr\n",
        "\n",
        "  for _, row in wordsim_df.iterrows():\n",
        "      w1, w2, human_sim = row['Word 1'], row['Word 2'], row['Human (Mean)']\n",
        "\n",
        "      if w1 in word2idx and w2 in word2idx:\n",
        "          emb1 = get_embedding(w1, model, word2idx)\n",
        "          emb2 = get_embedding(w2, model, word2idx)\n",
        "\n",
        "          sim = cosine_similarity(emb1, emb2)\n",
        "          human_scores.append(human_sim)\n",
        "          model_scores.append(sim)\n",
        "      else:\n",
        "          pass\n",
        "\n",
        "  corr, _ = spearmanr(human_scores, model_scores)\n",
        "  results[\"wordsim\"] = corr\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Od9lyap_AYtI"
      },
      "outputs": [],
      "source": [
        "def build_vocab(corpus, min_freq=1):\n",
        "    counter = Counter()\n",
        "    for sentence in corpus:\n",
        "        counter.update(sentence)\n",
        "    vocab = {w for w, c in counter.items() if c >= min_freq}\n",
        "    word2idx = {w: i for i, w in enumerate(sorted(vocab))}\n",
        "    idx2word = {i: w for w, i in word2idx.items()}\n",
        "    return word2idx, idx2word\n",
        "\n",
        "def build_cooccurrence(corpus, word2idx, window_size=5):\n",
        "    cooccurrences = defaultdict(float)\n",
        "    for sentence in corpus:\n",
        "        indices = [word2idx[w] for w in sentence if w in word2idx]\n",
        "        for center_i, center_word in enumerate(indices):\n",
        "            start = max(0, center_i - window_size)\n",
        "            end = min(len(indices), center_i + window_size + 1)\n",
        "            for context_i in range(start, end):\n",
        "                if context_i != center_i:\n",
        "                    context_word = indices[context_i]\n",
        "                    dist = abs(center_i - context_i)\n",
        "                    cooccurrences[(center_word, context_word)] += 1.0 / dist\n",
        "    return cooccurrences\n",
        "\n",
        "def weighting_func(x, x_max=100, alpha=0.75):\n",
        "    return torch.where(x < x_max, (x / x_max) ** alpha, torch.ones_like(x))\n",
        "\n",
        "class GloVe(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.wi = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.wj = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.bi = nn.Embedding(vocab_size, 1)\n",
        "        self.bj = nn.Embedding(vocab_size, 1)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.wi.weight)\n",
        "        nn.init.xavier_uniform_(self.wj.weight)\n",
        "        nn.init.zeros_(self.bi.weight)\n",
        "        nn.init.zeros_(self.bj.weight)\n",
        "\n",
        "    def forward(self, i_idx, j_idx):\n",
        "        w_i = self.wi(i_idx)\n",
        "        w_j = self.wj(j_idx)\n",
        "        b_i = self.bi(i_idx).squeeze()\n",
        "        b_j = self.bj(j_idx).squeeze()\n",
        "        x = (w_i * w_j).sum(dim=1) + b_i + b_j\n",
        "        return x\n",
        "\n",
        "def train_glove(corpus, embedding_dim=256, window_size=5, epochs=50, lr=0.005):\n",
        "    word2idx, idx2word = build_vocab(corpus)\n",
        "    cooccurrences = build_cooccurrence(corpus, word2idx, window_size)\n",
        "\n",
        "    i_idx = torch.tensor([i for (i, j) in cooccurrences.keys()], dtype=torch.long)\n",
        "    j_idx = torch.tensor([j for (i, j) in cooccurrences.keys()], dtype=torch.long)\n",
        "    counts = torch.tensor([v for v in cooccurrences.values()], dtype=torch.float32)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = GloVe(len(word2idx), embedding_dim).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    i_idx = i_idx.to(device)\n",
        "    j_idx = j_idx.to(device)\n",
        "    counts = counts.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        total_loss = 0\n",
        "        preds = model(i_idx, j_idx)\n",
        "        log_counts = torch.log(counts + 1e-8)\n",
        "        weights = weighting_func(counts)\n",
        "\n",
        "        loss = (weights * (preds - log_counts) ** 2).mean()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "            results = evaluate_benchmarks(model, word2idx)\n",
        "            print(\"wordsim: \", results[\"wordsim\"])\n",
        "            print(\"simverb: \", results[\"simverb\"])\n",
        "\n",
        "            wandb.log({\n",
        "              \"epoch\": epoch+1,\n",
        "              \"train_loss\": total_loss,\n",
        "              \"wordsim_corr\": results[\"wordsim\"],\n",
        "              \"simverb_corr\": results[\"simverb\"]\n",
        "            })\n",
        "\n",
        "        embeddings = model.wi.weight.data + model.wj.weight.data\n",
        "\n",
        "\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "XHV-qpfN-vVf",
        "outputId": "d8c5039c-a339-4338-8251-5de3ec1eb574"
      },
      "outputs": [],
      "source": [
        "!wandb login\n",
        "cfg = {k: v for k, v in vars(CFG).items() if not k.startswith('__') and not callable(v)}\n",
        "wandb.init(project=\"CLIP\", config=cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XKtKHDfsAlms",
        "outputId": "3c303cd3-460a-4ef9-b483-419a9b069c81"
      },
      "outputs": [],
      "source": [
        "train_glove(corpus, embedding_dim=CFG.emb_size, epochs=CFG.train_epochs)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
